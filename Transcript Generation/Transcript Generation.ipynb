{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVQXC_dSs44_"
   },
   "source": [
    "#  Transcript Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2cBVwfds45L"
   },
   "source": [
    "# Section 0: Setup/Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24885,
     "status": "ok",
     "timestamp": 1626814902427,
     "user": {
      "displayName": "Jacob Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjC-yKvzVTb1zobawVkaFHsEIBIEkxI9kqWZX7pzg=s64",
      "userId": "16407710070398725275"
     },
     "user_tz": 420
    },
    "id": "NfYu7R3us45L",
    "outputId": "0e4aacb4-3bde-4af6-b383-2a6f19524303"
   },
   "outputs": [],
   "source": [
    "# TODO: Run this cell and follow instructions to connect this notebook to Google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except ImportError:\n",
    "    print(\"Not on google drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 166,
     "status": "ok",
     "timestamp": 1626815083218,
     "user": {
      "displayName": "Jacob Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjC-yKvzVTb1zobawVkaFHsEIBIEkxI9kqWZX7pzg=s64",
      "userId": "16407710070398725275"
     },
     "user_tz": 420
    },
    "id": "YvVZOd0mvAB2",
    "outputId": "cb1bd3b9-ad81-4074-8398-b14b17f141c5"
   },
   "outputs": [],
   "source": [
    "# TODO: Replace path below to the folder containing your notebook and data folder\n",
    "%cd path/to/folder/in/google/drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Run this cell to download the data from Amazon AWS\n",
    "# TODO If needed, replace your the local Google Drive path (/content/drive/MyDrive/pa1b/) with a path that works for you \n",
    "\n",
    "!wget -P /content/drive/MyDrive/pa3b/ https://cmu-dele-leaderboard-us-east-2-003014019879.s3.us-east-2.amazonaws.com/colab/pa3b/data3pb.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Run this cell to unzip the data from Amazon AWS to your local Drive\n",
    "# TODO If needed, replace your the local Google Drive path (/content/drive/MyDrive/pa1b/data1pb.zip) with a path that works for you \n",
    "\n",
    "\n",
    "!unzip /content/drive/MyDrive/pa3b/data3pb.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the levenshtein distance package\n",
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 154,
     "status": "ok",
     "timestamp": 1626815198874,
     "user": {
      "displayName": "Jacob Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjC-yKvzVTb1zobawVkaFHsEIBIEkxI9kqWZX7pzg=s64",
      "userId": "16407710070398725275"
     },
     "user_tz": 420
    },
    "id": "Zqq0_ZBSs45H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.1 3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Run this cell to import packages\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from Levenshtein import distance\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pack_sequence\n",
    "print(torch.__version__, sys.version)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EQh4445Wtkfg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# TODO: Run this cell to automatically detect if GPU is available.\n",
    "DEVICE = torch.device('cuda' if not torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load in our data and preview it.\n",
    "\n",
    "You can see the `TOKEN_LIST` in the `utils.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first load in the data and label files.\n",
    "from utils import load_data, convert_str_to_idxs\n",
    "\n",
    "# TODO: If necessary, change the strings below to be the paths of your data files. \n",
    "train_data_path = \"data/train_data.npy\"\n",
    "train_labels_path = \"data/train_labels.npy\"\n",
    "val_data_path = \"data/val_data.npy\"\n",
    "val_labels_path = \"data/val_labels.npy\"\n",
    "test_data_path = \"data/test_data.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example data:\n",
      "[[2.3062422e-03 2.0986800e-03 3.0386688e-03 ... 4.5379176e-05\n",
      "  1.3997178e-05 1.1967877e-05]\n",
      " [3.1049701e-03 7.7780215e-03 2.2952498e-03 ... 9.4744173e-06\n",
      "  4.0979262e-06 6.3597912e-07]\n",
      " [2.9091453e-03 8.3023859e-03 2.6321730e-03 ... 1.2208303e-05\n",
      "  4.4432700e-06 5.5138651e-07]\n",
      " ...\n",
      " [2.5126212e-03 8.6815646e-03 3.5488086e-03 ... 1.7203306e-06\n",
      "  3.8962827e-07 3.2171218e-07]\n",
      " [2.3398087e-03 7.0686312e-03 2.7115962e-03 ... 1.7138029e-06\n",
      "  1.0856153e-06 6.4771206e-07]\n",
      " [2.9576111e-03 4.1935476e-03 7.5066503e-04 ... 2.4030285e-06\n",
      "  1.0650157e-06 4.4126014e-07]]\n",
      "Shape of this data (num_frames, num_channels):\n",
      "(307, 40)\n",
      "Label:\n",
      "THIS IS THE AMUSING ADVENTURE WHICH CLOSED OUR EXPLOITS\n",
      "Label converted to list(int) with <SOS> and <EOS> indices added:\n",
      "[37, 20, 8, 9, 19, 36, 9, 19, 36, 20, 8, 5, 36, 1, 13, 21, 19, 9, 14, 7, 36, 1, 4, 22, 5, 14, 20, 21, 18, 5, 36, 23, 8, 9, 3, 8, 36, 3, 12, 15, 19, 5, 4, 36, 15, 21, 18, 36, 5, 24, 16, 12, 15, 9, 20, 19, 38]\n",
      "Number of tokens in label:\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "# TODO: Run this cell to preview what your data and labels will look like. \n",
    "val_data, val_labels = load_data(val_data_path, val_labels_path)\n",
    "\n",
    "print(f\"Example data:\\n{val_data[0]}\")\n",
    "print(f\"Shape of this data (num_frames, num_channels):\\n{val_data[0].shape}\")\n",
    "print(f\"Label:\\n{val_labels[0]}\")\n",
    "print(f\"Label converted to list(int) with <SOS> and <EOS> indices added:\\n{convert_str_to_idxs(val_labels[0])}\")\n",
    "print(f\"Number of tokens in label:\\n{len(convert_str_to_idxs(val_labels[0]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Let's begin by writing our own custom `Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data\n",
    "\n",
    "class Speech2TextDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for training a speech-to-text model.\"\"\"\n",
    "    def __init__(self, data_path, labels_path=None):\n",
    "        \"\"\"[Given] All data preprocessing (including converting to Tensors) should happen here.\n",
    "        This method runs only once, when the object is instantialized.\n",
    "        \n",
    "        You technically could do more processing/conversion in the __getitem__() method,\n",
    "        but it'd drastically slow down querying data.  \n",
    "\n",
    "        Args:\n",
    "            data_path (str): Path to *_data.npy file\n",
    "            labels_path (str, optional): Path to *_labels.npy file. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Load in data (and labels, if given)\n",
    "        if labels_path is not None:\n",
    "            data, labels = load_data(data_path, labels_path)\n",
    "        else:\n",
    "            data = load_data(data_path)\n",
    "            labels = None\n",
    "        \n",
    "        # Convert the data to FloatTensors\n",
    "        self.data = [torch.FloatTensor(d) for d in data]\n",
    "        \n",
    "        # Convert the labels to index tensors\n",
    "        if labels is not None:\n",
    "            self.labels = [torch.LongTensor(convert_str_to_idxs(l)) for l in labels]\n",
    "        else:\n",
    "            self.labels = None\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" TODO: This method defines what happens when someone runs len() on this object.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of observations in the dataset.\n",
    "        \"\"\"\n",
    "        # TODO: Complete this method based on the docstring above (1-liner, don't overthink)\n",
    "        ### BEGIN SOLUTION\n",
    "        return len(self.data)\n",
    "        ### END SOLUTION\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" TODO: This method defines what happens when someone tries to index this object, e.g. `train_dataset[3]`\n",
    "\n",
    "        Args:\n",
    "            idx (int): The idx of the desired observation from self.data and self.labels (if exists). Will be in [0, len(self))\n",
    "                       After defining this method, multi-index querying such as `train_dataset[3:5]` will work too.\n",
    "\n",
    "        Returns (depends on if labels are given):\n",
    "            torch.FloatTensor, torch.LongTensor: If labels given, return data and labels\n",
    "            or \n",
    "            torch.FloatTensor: If no labels given, return only data\n",
    "        \"\"\"\n",
    "        # TODO: Complete this method based on the docstring above\n",
    "        # Hint: check if `self.labels` exists\n",
    "        ### BEGIN SOLUTION\n",
    "        if self.labels is not None:\n",
    "            return self.data[idx], self.labels[idx]\n",
    "        else:\n",
    "            return self.data[idx]\n",
    "        ### END SOLUTION\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test out your implementation with the val and test datasets to make sure everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run to test the __init__ method.\n",
    "val_dataset = Speech2TextDataset(val_data_path, val_labels_path)\n",
    "test_dataset = Speech2TextDataset(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(val_dataset): 2703\n",
      "len(test_dataset): 2620\n"
     ]
    }
   ],
   "source": [
    "# TODO: Run to test the __len__ method\n",
    "assert len(val_dataset) == 2703, \"__len__ method defined incorrectly, or paths to val files are incorrect\"\n",
    "assert len(test_dataset) == 2620, \"__len__ method defined incorrectly, or paths to test file is incorrect\"\n",
    "print(f\"len(val_dataset): {len(val_dataset)}\")\n",
    "print(f\"len(test_dataset): {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything works correctly!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Run to test the __getitem__ method\n",
    "\n",
    "# Test that querying works on the val dataset\n",
    "data, label = val_dataset[0]\n",
    "assert data is not None and label is not None, \"__getitem__ defined incorrectly, val dataset shouldn't return None for labels\"\n",
    "assert isinstance(data, torch.Tensor) and isinstance(label, torch.Tensor), \"Objects returned are not tensors.\"\n",
    "assert data.shape == (307, 40), \"Shape of queried data is incorrect, possibly queried wrong data\"\n",
    "assert label.shape == (57,), \"Shape of queried label is incorrect, possibly queried wrong label\"\n",
    "\n",
    "# Test that querying works on the test dataset\n",
    "data = test_dataset[0]\n",
    "assert isinstance(data, torch.Tensor), f\"Test dataset should return only a single data tensor\"\n",
    "\n",
    "print(\"Everything works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `collate_and_pad`\n",
    "Below, we give you the implementation of the collate function we described in the writeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_and_pad(batch):\n",
    "    \"\"\"Instructions for the dataloader on how to form a batch given multiple observations\n",
    "    \n",
    "    Args:\n",
    "        batch (list): list of observations. If labels are present, it will be a list of tuples of two tensors,\n",
    "                      else it'll be a list of tensors\n",
    "                      \n",
    "    Returns (depends on if labels are present):\n",
    "        torch.FloatTensor, torch.LongTensor, torch.FloatTensor: data, data_lens, labels\n",
    "        or \n",
    "        torch.FloatTensor, torch.LongTensor: data, data_lens\n",
    "    \"\"\"\n",
    "    # If each item in batch is a tuple, that means labels are present\n",
    "    if isinstance(batch[0], tuple):\n",
    "        # Convert the list of (data, label) into two separate lists\n",
    "        data, labels = zip(*batch)\n",
    "        \n",
    "        # Pad the labels and make into a single tensor\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=0)\n",
    "    else:\n",
    "        data, labels = batch, None\n",
    "        \n",
    "    # Create tensors for lengths and padded inputs, similar to above\n",
    "    data_lens = torch.LongTensor([len(d) for d in data])\n",
    "    data = torch.nn.utils.rnn.pad_sequence(data, batch_first=True, padding_value=0)\n",
    "    \n",
    "    if labels is not None:\n",
    "        return data, data_lens, labels\n",
    "    else:\n",
    "        return data, data_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgBtE_Jas45M"
   },
   "source": [
    "## Initialize `Dataset`s and `DataLoader`s \n",
    "Next we'll initialize the custom `Dataset`s for train/val/test and the default `DataLoader`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Speech2TextDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d2ff67a4981e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m### BEGIN SOLUTION\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSpeech2TextDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSpeech2TextDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSpeech2TextDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Speech2TextDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: Initialize Speech2TextDataset objects here\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "train_dataset = Speech2TextDataset(train_data_path, train_labels_path)\n",
    "val_dataset = Speech2TextDataset(val_data_path, val_labels_path)\n",
    "test_dataset = Speech2TextDataset(test_data_path)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2dd31d6ee1ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m### BEGIN SOLUTION\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mtrain_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_and_pad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mval_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_and_pad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_and_pad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jacob\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[0;32m    268\u001b[0m                     \u001b[1;31m# Cannot statically verify that dataset is Sized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m                     \u001b[1;31m# Somewhat related: see NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m                     \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jacob\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m     99\u001b[0m                              \"since a random permute will be performed.\")\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0;32m    103\u001b[0m                              \"value, but got num_samples={}\".format(self.num_samples))\n",
      "\u001b[1;32mc:\\Users\\jacob\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\sampler.py\u001b[0m in \u001b[0;36mnum_samples\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m# dataset size might change at runtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_samples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_source\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Feel free to adjust based on guidelines we provided in homework 1B.\n",
    "batch_size = 64\n",
    "\n",
    "# TODO: Initialize dataloaders\n",
    "num_workers = 0\n",
    "# num_workers = os.cpu_count() # this will speed things up\n",
    "\n",
    "train_dataloader = None\n",
    "val_dataloader = None\n",
    "test_dataloader = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_and_pad)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_and_pad)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_and_pad)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `downsample()`\n",
    "In preparation for implementing the `pBLSTM`, let's first implement the downsampling operation that each `pBLSTM` performs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(x, lens):\n",
    "    \"\"\"Downsamples given input for pBLSTM.\n",
    "\n",
    "    Args:\n",
    "        x (torch.FloatTensor): (batch_size, seq_len, hidden_size) Data to downsample \n",
    "        lens (torch.LongTensor): (batch_size,) Length of each batch before padding\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor, torch.LongTensor: (batch_size, seq_len//2, hidden_size*2), (batch_size,)\n",
    "                                             x and lens after downsampling\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, hidden_size = x.shape\n",
    "\n",
    "    # TODO: Implement based on description in writeup\n",
    "    ### BEGIN SOLUTION\n",
    "    x = x[:, :seq_len//2*2, :]\n",
    "    x = x.reshape(batch_size, seq_len//2, hidden_size*2)\n",
    "    lens //= 2\n",
    "    return x, lens\n",
    "    ### END SOLUTION\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before downsampling: torch.Size([2, 5, 4])\n",
      "After downsampling: torch.Size([2, 2, 8])\n",
      "Correct!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-ab2c36194b3e>:18: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:450.)\n",
      "  lens //= 2\n"
     ]
    }
   ],
   "source": [
    "# TODO: Run this test\n",
    "\n",
    "# Example input shaped (batch_size=2, max_len=5, hidden_size=4)\n",
    "x = torch.FloatTensor([[[ 4.,  2.,  2.,  1.],  # First seq in the batch, a sequence of 5 frames, \n",
    "                        [ 2.,  2.,  1.,  -2.], # each with 4 frequency bands\n",
    "                        [ 1.,  3.,  3.,  2.],\n",
    "                        [ 3.,  2.,  2.,  4.],\n",
    "                        [ -2.,  1.,  1.,  1.]],\n",
    "\n",
    "                       [[ 2.,  1.,  -3., -1.], # Second seq in the batch, originally shaped (3, 4)\n",
    "                        [-2.,  1.,  3.,  2.],  # but padded with 0's to shape (5, 4) \n",
    "                        [ -2., -1.,  -1.,  3.],\n",
    "                        [ 0.,  0.,  0.,  0.],\n",
    "                        [ 0.,  0.,  0.,  0.]]])\n",
    "\n",
    "# Corresponding lengths tensor shaped (batch_size=2,)\n",
    "lens = torch.LongTensor([5, 3])\n",
    "\n",
    "# Run your downsampling method\n",
    "downsampled_x, downsampled_lens = downsample(x, lens)\n",
    "\n",
    "# Make sure input is correctly downsampled\n",
    "assert torch.equal(downsampled_x, torch.FloatTensor([[[ 4.,  2.,  2.,  1.,  2.,  2.,  1., -2.],\n",
    "                                                      [ 1.,  3.,  3.,  2.,  3.,  2.,  2.,  4.]],\n",
    "                                                    \n",
    "                                                     [[ 2.,  1., -3., -1., -2.,  1.,  3.,  2.],\n",
    "                                                      [-2., -1., -1.,  3.,  0.,  0.,  0.,  0.]]]))\n",
    "# Make sure lengths are correctly downsampled\n",
    "assert torch.equal(downsampled_lens, torch.LongTensor([2, 1]))\n",
    "\n",
    "print(\"Before downsampling:\", x.shape)\n",
    "print(\"After downsampling:\", downsampled_x.shape)\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pBLSTM`\n",
    "Now let's implement the custom object itself.\n",
    "\n",
    "Finish the `__init__` and `forward` methods.\n",
    "\n",
    "See [this link](https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch) for an explanation of why we convert input tensors to `PackedSequence`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pBLSTM(nn.Module):\n",
    "    \"\"\"The Pyramidal Bi-LSTM layer, as per LAS\"\"\"\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        # TODO: Initialize LSTM with appropriate parameters (see encoder diagram in writeup)\n",
    "        self.lstm = None \n",
    "        ### BEGIN SOLUTION\n",
    "        self.lstm = nn.LSTM(hidden_size*4, hidden_size, bidirectional=True, batch_first=True)\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the pBLSTM.\n",
    "\n",
    "        Args:\n",
    "            x (torch.nn.utils.rnn.PackedSequence): Packed input data.\n",
    "\n",
    "        Returns:\n",
    "            torch.nn.utils.rnn.PackedSequence: Packed output data.\n",
    "        \"\"\"\n",
    "        # [Given] Unpack the input\n",
    "        x, lens = pad_packed_sequence(x, batch_first=True)\n",
    "\n",
    "        # TODO: Run downsampling\n",
    "        ### BEGIN SOLUTION\n",
    "        x, lens = downsample(x, lens)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        # [Given] Pack the downsampled input\n",
    "        x = pack_padded_sequence(x, lens, enforce_sorted=False, batch_first=True)\n",
    "\n",
    "        # TODO: Run through the LSTM and return\n",
    "        ### BEGIN SOLUTION\n",
    "        x, _ = self.lstm(x)\n",
    "        return x\n",
    "        ### END SOLUTION\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a basic test for your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before pBLSTM: torch.Size([2, 5, 4])\n",
      "Shape after pBLSTM: torch.Size([2, 2, 4])\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Run this cell to test pBLSTM implementation\n",
    "from utils import init_pblstm_for_testing\n",
    "\n",
    "# Create layer\n",
    "pblstm = pBLSTM(hidden_size=2) # Note the hidden_size\n",
    "init_pblstm_for_testing(pblstm)\n",
    "\n",
    "# Create input shaped (batch_size=2, max_len=5, hidden_size=4)\n",
    "x = torch.FloatTensor([[[ 4.,  2.,  2.,  1.],\n",
    "                        [ 2.,  2.,  1.,  -2.],\n",
    "                        [ 1.,  3.,  3.,  2.],\n",
    "                        [ 3.,  2.,  2.,  4.],\n",
    "                        [ -2.,  1.,  1.,  1.]],\n",
    "\n",
    "                       [[ 2.,  1.,  -3., -1.],\n",
    "                        [-2.,  1.,  3.,  2.],\n",
    "                        [ -2., -1.,  -1.,  3.],\n",
    "                        [ 0.,  0.,  0.,  0.],\n",
    "                        [ 0.,  0.,  0.,  0.]]])\n",
    "\n",
    "# Create lengths tensor shaped (batch_size=2,)\n",
    "lens = torch.LongTensor([5, 3])\n",
    "\n",
    "# We need to pack this tensor before giving it to the layer\n",
    "print(\"Shape before pBLSTM:\", x.shape)\n",
    "x = pack_padded_sequence(x, lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "# Run through layer, unpack\n",
    "out = pblstm(x)\n",
    "out, lens = pad_packed_sequence(out, batch_first=True)\n",
    "\n",
    "print(\"Shape after pBLSTM:\", out.shape)\n",
    "\n",
    "out_expected = torch.tensor([\n",
    "    [[7.6159e-01, 7.6159e-01, 9.6403e-01, 9.6403e-01],\n",
    "     [9.6403e-01, 9.6403e-01, 7.6159e-01, 7.6159e-01]],\n",
    "    [[1.7026e-02, 9.1105e-04, 7.5950e-01, 1.0450e-01],\n",
    "     [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]])\n",
    "\n",
    "assert out.shape == (2, 2, 4), \"Shape of the output is incorrect; did you return the correct one?\"\n",
    "assert torch.equal(lens, torch.tensor([2, 1])), \"Lens tensor is incorrect; did you return the correct downsampled one?\"\n",
    "assert torch.allclose(out, out_expected, atol=1e-4), \"Output is incorrect; did you correctly instantiate your LSTM?\"\n",
    "\n",
    "print(\"Tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Encoder`\n",
    "\n",
    "Now to implement the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"The Encoder embeds input speech data by projecting them into a 'key' tensor and 'value' tensor.\"\"\"\n",
    "    def __init__(self, num_channels, hidden_size, attn_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # TODO: Initialize layers appropriately using the args given to __init__\n",
    "        self.lstm = None\n",
    "        self.pblstm1 = None\n",
    "        self.pblstm2 = None\n",
    "        self.pblstm3 = None\n",
    "        self.key_network = None\n",
    "        self.value_network = None\n",
    "        ### BEGIN SOLUTION\n",
    "        self.lstm = nn.LSTM(num_channels, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.pblstm1 = pBLSTM(hidden_size)\n",
    "        self.pblstm2 = pBLSTM(hidden_size)\n",
    "        self.pblstm3 = pBLSTM(hidden_size)\n",
    "        self.key_network = nn.Linear(hidden_size*2, attn_size)\n",
    "        self.value_network = nn.Linear(hidden_size*2, attn_size)\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def forward(self, x, lens):\n",
    "        \"\"\"Forward pass of the LAS encoder\n",
    "\n",
    "        Args:\n",
    "            x (torch.FloatTensor): Padded input tensor, before packing. Shaped (batch_size, num_frames, num_channels)\n",
    "            lens (torch.LongTensor): Lengths of each seq before padding. Shaped (batch_size,)\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor, torch.FloatTensor, torch.LongTensor : keys, values, lens\n",
    "        \"\"\"\n",
    "        # [Given] Pack sequence\n",
    "        x = pack_padded_sequence(x, lengths=lens.cpu(), enforce_sorted=False, batch_first=True)\n",
    "        \n",
    "        # TODO: Pass through LSTM and pBLSTMs\n",
    "        ### BEGIN SOLUTION\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.pblstm1(x)\n",
    "        x = self.pblstm2(x)\n",
    "        x = self.pblstm3(x)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        # [Given] Unpack\n",
    "        x, lens = pad_packed_sequence(x, batch_first=True)\n",
    "        \n",
    "        # TODO: Pass through final linear layers, return\n",
    "        ### BEGIN SOLUTION\n",
    "        keys = self.key_network(x)\n",
    "        values = self.value_network(x)\n",
    "        return keys, values, lens\n",
    "        ### END SOLUTION\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a simple test to see if your encoder will initialize and pass an input through successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape: torch.Size([2, 18, 5]), data_lens.shape: torch.Size([2])\n",
      "keys.shape: torch.Size([2, 2, 2]), values.shape: torch.Size([2, 2, 2]), lens.shape: torch.Size([2])\n",
      "Seems good!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacob\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    }
   ],
   "source": [
    "from utils import init_encoder_for_testing\n",
    "\n",
    "# Initialize (for the actual encoder, use input_size 40, hidden_size 256, attn_size 128!)\n",
    "encoder = Encoder(num_channels=5, hidden_size=4, attn_size=2)\n",
    "init_encoder_for_testing(encoder)\n",
    "\n",
    "# Create some random data\n",
    "data = torch.randint(5, (2, 18, 5)).float()\n",
    "data_lens = torch.LongTensor([18, 16])\n",
    "print(f\"data.shape: {data.shape}, data_lens.shape: {data_lens.shape}\")\n",
    "\n",
    "# Pass through encoder\n",
    "keys, values, lens = encoder(data, data_lens)\n",
    "print(f\"keys.shape: {keys.shape}, values.shape: {values.shape}, lens.shape: {lens.shape}\")\n",
    "\n",
    "# Check that keys and values are correctly shaped\n",
    "assert keys.shape[1] == data.shape[1] // 8 and values.shape[1] == data.shape[1] // 8, \"seq_len dimension of keys and values not correctly shortened by // 8\"\n",
    "assert keys.shape[2] == 2 and values.shape[2] == 2, \"Keys and values should have last dimension size 4 (the attn_size we set), but it does not.\"\n",
    "\n",
    "# Check that the lengths are shortened too\n",
    "assert torch.equal(data_lens//8, lens), \"Values in the lens tensor are not correctly shortened by // 8\"\n",
    "\n",
    "# Check values of keys and values\n",
    "keys_expected = torch.tensor([\n",
    "    [[21.2562, 17.0434], [21.2562, 16.8409]],\n",
    "    [[21.2562, 17.0434], [21.2562, 16.8409]]])\n",
    "\n",
    "values_expected = torch.tensor([\n",
    "    [[14.6025, 14.6025], [15.0074, 15.0074]],\n",
    "    [[14.6025, 14.6025], [15.0074, 15.0074]]])\n",
    "\n",
    "assert torch.allclose(keys, keys_expected, atol = 1e-4), \"Keys are incorrect, 2x check your encoder!\"\n",
    "assert torch.allclose(values, values_expected, atol = 1e-4), \"Values are incorrect, 2x check your encoder!\"\n",
    "\n",
    "print(\"Seems good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: `Decoder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Attention`\n",
    "\n",
    "Let's first implement the attention mechanism, as it'll be needed in the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # [Optional] If desired, you init your own layers here to actively learn attention.\n",
    "\n",
    "    def forward(self, query, keys, values, lens):\n",
    "        \"\"\"Forward pass of attention.\n",
    "\n",
    "        Args:\n",
    "            query (torch.FloatTensor): (batch_size, attn_size)\n",
    "            keys (torch.FloatTensor): (batch_size, seq_len, attn_size)\n",
    "            values (torch.FloatTensor): (batch_size, seq_len, attn_size)\n",
    "            lens (torch.LongTensor): (batch_size,)\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor, torch.FloatTensor: context (batch_size, attn_size) and attention (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        # TODO: Implement steps 1-3 of attention diagram in writeup\n",
    "        scores = None\n",
    "        ### BEGIN SOLUTION\n",
    "        scores = torch.bmm(keys, query.unsqueeze(2)).squeeze(2)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        # [Given] Step 4\n",
    "        mask = torch.arange(values.size(1), device=DEVICE).unsqueeze(0) >= lens.to(DEVICE).unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "        attention = F.softmax(scores, dim=1)\n",
    "        \n",
    "        # TODO: Complete remaining steps \n",
    "        ### BEGIN SOLUTION\n",
    "        context = torch.bmm(attention.unsqueeze(1), values).squeeze(1)\n",
    "        return context, attention\n",
    "        ### END SOLUTION\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test your implementation of attention!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query.shape: torch.Size([2, 2]), key.shape: torch.Size([2, 2, 2]), value.shape: torch.Size([2, 2, 2]), lens.shape: torch.Size([2])\n",
      "context.shape: torch.Size([2, 2]), attention_mask.shape: torch.Size([2, 2])\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "# Initialize inputs\n",
    "query = torch.FloatTensor([[1, 2],\n",
    "                           [3, 4]]).to(DEVICE)\n",
    "key = torch.FloatTensor([[[3, -2],\n",
    "                          [1, 2]],\n",
    "                         [[4, 2],\n",
    "                          [2, 4]]]).to(DEVICE)\n",
    "value = torch.FloatTensor([[[1, 2],\n",
    "                          [2, 1]],\n",
    "                         [[-2, 2],\n",
    "                          [3, -2]]]).to(DEVICE)\n",
    "lens = torch.FloatTensor([1, 2]).to(DEVICE)\n",
    "\n",
    "print(f\"query.shape: {query.shape}, key.shape: {key.shape}, value.shape: {value.shape}, lens.shape: {lens.shape}\")\n",
    "\n",
    "# Initialize attention module, pass inputs through\n",
    "attention = Attention()\n",
    "context, attention_mask = attention(query, key, value, lens)\n",
    "\n",
    "print(f\"context.shape: {context.shape}, attention_mask.shape: {attention_mask.shape}\")\n",
    "\n",
    "expected_context = torch.FloatTensor([[ 1.0000,  2.0000], [ 2.4040, -1.5232]]).to(DEVICE)\n",
    "expected_attention_mask = torch.FloatTensor([[1.0000, 0.0000], [0.1192, 0.8808]]).to(DEVICE)\n",
    "\n",
    "# Check context vector values are close enough to reference (within floating point tolerance)\n",
    "assert torch.allclose(context, expected_context, atol=1e-4), \\\n",
    "        \"Values or shape of context is incorrect.\"\n",
    "\n",
    "# Check attention mask values\n",
    "assert torch.allclose(attention_mask, expected_attention_mask, atol=1e-4), \\\n",
    "        \"Values or shape of attention_mask is incorrect\"\n",
    "\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Decoder`\n",
    "\n",
    "Implement `forward` and (optional but recommended) `prepare_input`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import token_to_idx\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, attn_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # [Given] Initialize modules\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=hidden_size, padding_idx=0)\n",
    "        self.lstm1 = nn.LSTMCell(input_size=hidden_size + attn_size, hidden_size=hidden_size)\n",
    "        self.lstm2 = nn.LSTMCell(input_size=hidden_size, hidden_size=attn_size)\n",
    "        self.attention_layer = Attention()\n",
    "        self.character_prob = nn.Linear(attn_size*2, vocab_size)\n",
    "\n",
    "    def forward(self, keys, values, lens, labels, tf_prob):\n",
    "        \"\"\"Forward pass of decoder\n",
    "        \n",
    "        Args:\n",
    "            keys (torch.FloatTensor): (batch_size, seq_len, attn_size)\n",
    "            value (torch.FloatTensor): (batch_size, seq_len, attn_size)\n",
    "            lens (torch.LongTensor): (batch_size,)\n",
    "            labels (torch.LongTensor): Labels as indices, shaped (batch_size, max_label_len)\n",
    "                                       only needed during training. During eval, this should be None.\n",
    "            tf_prob (float): Teacher forcing probability, where 0 means we never give correct labels\n",
    "                                       and 1 is we always give correct labels. During eval, this should be 0.\n",
    "        Returns:\n",
    "            torch.FloatTensor, torch.FloatTensor: Concatenated predictions (batch_size, vocab_size, max_len)\n",
    "                                                  and stacked attentions (max_len, seq_len)\n",
    "        \"\"\"\n",
    "        # [Given] Depending on if we're in train or eval, set max_len and pre-generate label embeddings \n",
    "        if labels is not None: # Train\n",
    "            max_len = labels.shape[1] - 1\n",
    "            label_embeddings = self.embedding_layer(labels)\n",
    "        else:\n",
    "            max_len = 600 # Eval\n",
    "            label_embeddings = None\n",
    "        \n",
    "        # [Given] Initialize first prediction logit as having 100% probability of predicting <sos>\n",
    "        prediction = torch.zeros((keys.shape[0], self.vocab_size), dtype=torch.float, device=DEVICE)\n",
    "        prediction[:, token_to_idx[\"<sos>\"]] = 1.0\n",
    "        \n",
    "        # [Given] Initialize context vector\n",
    "        context = values[:, 0, :] # Normally this should store the attended values of attention,\n",
    "                                  # but at t=0 we just use a slice of values shaped (batch_size, attn_size)\n",
    "        \n",
    "        # [Given] Other initializations\n",
    "        predictions = [] # Append your predicted logit at each timestep here\n",
    "                         # Note we don't store the above <sos> prediction, not needed for loss calculation\n",
    "        hidden_states = [None, None] # Two sets of hidden states, one for each LSTMCell.\n",
    "                                     # Each list will hold the h_0 and c_0 of that cell to pass between time steps  \n",
    "        attentions = [] # To store the attention tensors produced at each time step\n",
    "\n",
    "        # TODO: Follow for loop pseudocode in writeup\n",
    "        ### BEGIN SOLUTION\n",
    "        for t in range(max_len):\n",
    "            x = self.prepare_input(prediction, label_embeddings, context, t, tf_prob)\n",
    "            \n",
    "            # Pass input through LSTMCells\n",
    "            hidden_states[0] = self.lstm1(x, hidden_states[0])\n",
    "            x = hidden_states[0][0]\n",
    "            hidden_states[1] = self.lstm2(x, hidden_states[1])\n",
    "            x = hidden_states[1][0] # (batch_size, hidden_size)\n",
    "            \n",
    "            # Attention\n",
    "            context, attention = self.attention_layer(x, keys, values, lens)\n",
    "            \n",
    "            # Calculate prediction logit for next token\n",
    "            prediction = self.character_prob(torch.cat([x, context], dim=1))\n",
    "            \n",
    "            # Append the attention mask and predictions\n",
    "            attentions.append(attention[0, :])\n",
    "            predictions.append(prediction)\n",
    "        ### END SOLUTION\n",
    "\n",
    "        # TODO: Return appropriate args\n",
    "        ### BEGIN SOLUTION\n",
    "        return torch.stack(predictions, dim=2), torch.stack(attentions)\n",
    "        ### END SOLUTION\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def prepare_input(self, prediction, label_embeddings, context, t, tf_prob):\n",
    "        \"\"\"[Optional] Method to prepare x at each timestep. Step 1 in for loop pseudocode. \n",
    "        \n",
    "        We made a separate method for this to reduce clutter, but you can implement step 1 directly in the for loop.\n",
    "\n",
    "        Args:\n",
    "            prediction (torch.FloatTensor): (batch_size, vocab_size) Prediction logit of previous timestep  \n",
    "            context (torch.FloatTensor): (batch_size, attn_size) Context from previous timestep\n",
    "            label_embeddings (torch.FloatTensor): (batch_size, hidden_size) Pre-embedded labels.\n",
    "                                                  During eval, this will be None.\n",
    "            t (int): Index of current timestep, used to index label_embeddings if teacher forcing\n",
    "            tf_prob (float): The probability of teacher forcing occurring\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: x (batch_size, hidden_size+attn_size)\n",
    "        \"\"\"\n",
    "        # TODO: Implement step 1 of the for loop pseudocode, with teacher forcing\n",
    "        ### BEGIN SOLUTION\n",
    "        # If we're teacher forcing this time:\n",
    "        if random.random() < tf_prob:\n",
    "            char_embed = label_embeddings[:, t, :]\n",
    "        else:\n",
    "            # Otherwise, use our previous prediction\n",
    "            char_embed = self.embedding_layer(prediction.argmax(dim=-1))\n",
    "\n",
    "        return torch.cat([char_embed, context], dim=1)\n",
    "        ### END SOLUTION\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some tests to validate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good!\n"
     ]
    }
   ],
   "source": [
    "from utils import init_decoder_for_testing, TOKEN_LIST\n",
    "\n",
    "# Initialize weights of network with random seed\n",
    "decoder = Decoder(vocab_size=len(TOKEN_LIST), hidden_size=256, attn_size=4).to(DEVICE)\n",
    "init_decoder_for_testing(decoder)\n",
    "\n",
    "# Create keys and values, both shaped (batch_size=2, max_len=5, attn_size=4)\n",
    "keys = torch.FloatTensor([[[ 4.,  2.,  2.,  1.],\n",
    "                        [ 2.,  2.,  1.,  -2.],\n",
    "                        [ 1.,  3.,  3.,  2.],\n",
    "                        [ 3.,  2.,  2.,  4.],\n",
    "                        [ -2.,  1.,  1.,  1.]],\n",
    "\n",
    "                       [[ 2.,  1.,  -3., -1.],\n",
    "                        [-2.,  1.,  3.,  2.],\n",
    "                        [ -2., -1.,  -1.,  3.],\n",
    "                        [ 0.,  0.,  0.,  0.],\n",
    "                        [ 0.,  0.,  0.,  0.]]]).to(DEVICE)\n",
    "\n",
    "values = torch.FloatTensor([[[ 4.,  2.,  2.,  1.],\n",
    "                        [ 2.,  2.,  1.,  -2.],\n",
    "                        [ 1.,  3.,  3.,  2.],\n",
    "                        [ 3.,  2.,  2.,  4.],\n",
    "                        [ -2.,  1.,  1.,  1.]],\n",
    "\n",
    "                       [[ 2.,  1.,  -3., -1.],\n",
    "                        [-2.,  1.,  3.,  2.],\n",
    "                        [ -2., -1.,  -1.,  3.],\n",
    "                        [ 0.,  0.,  0.,  0.],\n",
    "                        [ 0.,  0.,  0.,  0.]]]).to(DEVICE)\n",
    "\n",
    "labels = torch.LongTensor([[10, 2, 5, 3, 5],\n",
    "                           [10, 2, 5, 3, 5]]).to(DEVICE)\n",
    "\n",
    "# Lengths tensor and tf probability of 0 (always use prev prediction)\n",
    "lens = torch.LongTensor([2, 5])\n",
    "tf_prob = 0.\n",
    "\n",
    "# Run through decoder\n",
    "predictions, attentions = decoder(keys, values, lens, labels, tf_prob)\n",
    "\n",
    "# Compare a slice of your prediction tensor against a reference. We use a slice for visual clarity.\n",
    "your_prediction_slice = predictions[-1, -1, : ]\n",
    "answer_prediction_slice = torch.tensor([ 8.2803,  9.6490,  9.8506,  9.8782]).to(DEVICE)\n",
    "\n",
    "# Reference attention matrix\n",
    "answer_attentions = torch.tensor([[0.9897, 0.0103, 0.0000, 0.0000, 0.0000],\n",
    "         [0.9969, 0.0031, 0.0000, 0.0000, 0.0000],\n",
    "         [0.9975, 0.0025, 0.0000, 0.0000, 0.0000],\n",
    "         [0.9975, 0.0025, 0.0000, 0.0000, 0.0000]]).to(DEVICE)\n",
    "\n",
    "\n",
    "# Check that slice of prediction is correct\n",
    "assert torch.allclose(your_prediction_slice, answer_prediction_slice, atol=1e-4), \\\n",
    "    \"Slice of your predictions do not match our reference.\"\n",
    "\n",
    "# Check that attention matrix is correct\n",
    "assert torch.allclose(attentions, answer_attentions, atol=1e-4), \\\n",
    "    \"Attention matrix does not match our reference.\"\n",
    "\n",
    "print(\"All good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: `LAS`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LAS(nn.Module):\n",
    "    \"\"\"Listen, Attend, and Spell model (Chan, Jaitly, Le, Vinyals 2015)\"\"\"\n",
    "    def __init__(self, num_channels, vocab_size, hidden_size, attn_size):\n",
    "        \"\"\"[Given]\n",
    "        Args:\n",
    "            num_channels (int): How many frequency bands each frame of each spectrogram has\n",
    "            vocab_size (int): How many tokens are in your vocabulary\n",
    "            hidden_size (int): Size of various components throughout network.\n",
    "            attn_size (int): Number of dimensions your attention should work with.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_channels, hidden_size, attn_size)\n",
    "        self.decoder = Decoder(vocab_size, hidden_size, attn_size)\n",
    "\n",
    "    def forward(self, spectrograms, spectrogram_lens, labels=None, tf_prob=0.):\n",
    "        \"\"\"[Given]\n",
    "        Args:\n",
    "            spectrograms (torch.FloatTensor): (batch_size, num_frames, num_channels) Padded batch of spectrograms\n",
    "            spectrogram_lens (torch.LongTensor): (batch_size,) Length of each spectrogram before padding\n",
    "            labels (torch.LongTensor, optional): (batch_size, max_label_len) Padded batch of label indices. Defaults to None.\n",
    "            tf_prob (float, optional): Teacher forcing probability. Defaults to 0. Must be 0 during eval\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor, torch.FloatTensor: Predictions (batch_size, vocab_size, max_len)\n",
    "                                                  Attentions (max_len, seq_len)\n",
    "        \"\"\"\n",
    "        key, value, lens = self.encoder(spectrograms, spectrogram_lens)\n",
    "        predictions, attentions = self.decoder(key, value, lens, labels, tf_prob)\n",
    "        return predictions, attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Train/Val/Test loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `train_epoch`\n",
    "\n",
    "This will be a pretty typical training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import convert_idxs_to_str\n",
    "\n",
    "def train_epoch(model, optimizer, dataloader, tf_prob=1.):\n",
    "    \"\"\"Runs a single training epoch.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Your initialized LAS model.\n",
    "        optimizer (torch.optim.Optimizer): An initialized optimizer.\n",
    "        dataloader (torch.utils.data.DataLoader): Your train dataloader\n",
    "        tf_prob (float, optional): Teacher forcing rate. Defaults to 1 (100%).\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: The final attention tensor of the epoch, shaped (max_len, seq_len)\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    model.train()\n",
    "    loss_per_batch = []\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    for i, (data, data_lens, labels) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        data, labels = data.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        predictions, attention = model(data, data_lens, labels, tf_prob)\n",
    "        loss = loss_function(predictions, labels[:, 1:])\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n",
    "        optimizer.step()\n",
    "        loss_per_batch.append(loss.item())\n",
    "    ### END SOLUTION\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we provide validation code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    \"\"\"Runs a single validation epoch and prints results.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Your initialized LAS model.\n",
    "        dataloader (torch.utils.data.DataLoader): Your val dataloader\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    distances = []\n",
    "    with torch.inference_mode():\n",
    "        for (data, data_lens, labels) in tqdm(dataloader, total=len(dataloader)):\n",
    "            data, labels = data.to(DEVICE), labels.to(DEVICE)\n",
    "            predictions, _ = model(data, data_lens, labels=None, tf_prob=0.)\n",
    "            pred_idxs = predictions.argmax(dim=1)\n",
    "            prediction_strs = [convert_idxs_to_str(p.tolist(), remove_special_tokens=True) for p in pred_idxs]\n",
    "            label_strs = [convert_idxs_to_str(l.tolist(), remove_special_tokens=True) for l in labels]\n",
    "            batch_distances = [distance(p, l) for p, l in zip(prediction_strs, label_strs)]\n",
    "            distances.extend(batch_distances)\n",
    "    print(f\"Example prediction: {prediction_strs[0]}\")\n",
    "    print(f\"Label: {label_strs[0]}\")\n",
    "    print(f\"Average Levenshtein distance: {np.mean(distances)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `predict`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    \"\"\"Generates list of predictions for a dataloader\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Your initialized LAS model.\n",
    "        dataloader (torch.utils.data.DataLoader): Your test dataloader\n",
    "    \n",
    "    Returns:\n",
    "        list: All prediction strings of the given test dataloader, in original order. \n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    model.eval()\n",
    "    prediction_strs = []\n",
    "    with torch.inference_mode():\n",
    "        for (data, data_lens) in tqdm(dataloader, total=len(dataloader)):\n",
    "            data = data.to(DEVICE)\n",
    "            predictions, _ = model(data, data_lens, labels=None, tf_prob=0.)\n",
    "            pred_idxs = predictions.argmax(dim=1)\n",
    "            batch_prediction_strs = [convert_idxs_to_str(p.tolist(), remove_special_tokens=True) for p in pred_idxs]\n",
    "            prediction_strs.extend(batch_prediction_strs)\n",
    "    return prediction_strs\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Initialization and Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "First, initialize the objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize your model (put on GPU), optimizer, and (optional) scheduler\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "model = LAS(num_channels=40, vocab_size=len(TOKEN_LIST), hidden_size=256, attn_size=128).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll make sure that predict runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predict(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "Now, write the full train loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_attention\n",
    "\n",
    "# TODO: Run for some number of epochs\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "num_epochs = 2\n",
    "for e in range(num_epochs):\n",
    "    print(f\"Epoch #{e}\")\n",
    "    attention = train_epoch(model, optimizer, train_dataloader, tf_prob=0.9)\n",
    "    plot_attention(attention)\n",
    "    validate(model, val_dataloader)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITPLwKKzs45X"
   },
   "source": [
    "# Section 7: Test Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to generate predictions and export!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oo2bEmc9s45Y"
   },
   "outputs": [],
   "source": [
    "from utils import export_predictions_to_csv\n",
    "\n",
    "predictions = predict(model, test_dataloader)\n",
    "export_predictions_to_csv(predictions)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "-2cBVwfds45L",
    "6ORcUUwWvZOu",
    "6kP0mE8gs45G",
    "mv7pNwQl51uh"
   ],
   "name": "pa_01_B_assignment.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c79f8f21da264bd1d028db25c6791996e084443b727ae68f20d92653727834ef"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
