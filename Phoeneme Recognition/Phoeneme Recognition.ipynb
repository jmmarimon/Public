{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVQXC_dSs44_"
   },
   "source": [
    "# Phoneme Recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2cBVwfds45L"
   },
   "source": [
    "## Setup/Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24885,
     "status": "ok",
     "timestamp": 1626814902427,
     "user": {
      "displayName": "Jacob Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjC-yKvzVTb1zobawVkaFHsEIBIEkxI9kqWZX7pzg=s64",
      "userId": "16407710070398725275"
     },
     "user_tz": 420
    },
    "id": "NfYu7R3us45L",
    "outputId": "0e4aacb4-3bde-4af6-b383-2a6f19524303"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not on google drive\n"
     ]
    }
   ],
   "source": [
    "# TODO: Run this cell and follow instructions to connect this notebook to Google Drive\n",
    "# Additional guidance: https://colab.research.google.com/notebooks/io.ipynb\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except ImportError:\n",
    "    print(\"Not on google drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 166,
     "status": "ok",
     "timestamp": 1626815083218,
     "user": {
      "displayName": "Jacob Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjC-yKvzVTb1zobawVkaFHsEIBIEkxI9kqWZX7pzg=s64",
      "userId": "16407710070398725275"
     },
     "user_tz": 420
    },
    "id": "YvVZOd0mvAB2",
    "outputId": "cb1bd3b9-ad81-4074-8398-b14b17f141c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: 'path/to/folder/in/google/drive'\n",
      "C:\\Users\\adams\\OneDrive\\Documents\\GitHub\\cmu-dele\\assignments\\pa1b\n"
     ]
    }
   ],
   "source": [
    "# TODO: Change directories (\"cd\") to the folder containing your\n",
    "# notebook and data folder by replacing the filepath below\n",
    "%cd path/to/folder/in/google/drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Run this cell to download the data from Amazon AWS\n",
    "# TODO If needed, replace your the local Google Drive path (/content/drive/MyDrive/pa1b/) with a path that works for you \n",
    "\n",
    "!wget -P /content/drive/MyDrive/pa1b/ https://cmu-dele-leaderboard-us-east-2-003014019879.s3.us-east-2.amazonaws.com/colab/pa1b/data1pb.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Run this cell to unzip the data from Amazon AWS to your local Drive\n",
    "# TODO If needed, replace your the local Google Drive path (/content/drive/MyDrive/pa1b/data1pb.zip) with a path that works for you \n",
    "\n",
    "\n",
    "!unzip /content/drive/MyDrive/pa1b/data1pb.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 154,
     "status": "ok",
     "timestamp": 1626815198874,
     "user": {
      "displayName": "Jacob Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjC-yKvzVTb1zobawVkaFHsEIBIEkxI9kqWZX7pzg=s64",
      "userId": "16407710070398725275"
     },
     "user_tz": 420
    },
    "id": "Zqq0_ZBSs45H"
   },
   "outputs": [],
   "source": [
    "# TODO: Run this cell to import packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mv7pNwQl51uh"
   },
   "source": [
    "### Auto-detect if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQh4445Wtkfg"
   },
   "outputs": [],
   "source": [
    "# TODO: Run this cell to automatically detect if GPU is available.\n",
    "# Output should be 'cuda' if you are expecting to be on a GPU\n",
    "# If the output is equal to 'cpu', click on 'Runtime' in the top menu, then 'Change Runtime Type', and select 'GPU'\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgBtE_Jas45M"
   },
   "source": [
    "# Section 1: `Dataset`/`DataLoader`\n",
    "\n",
    "When working with any dataset in `torch`, you'll almost always work with a `Dataset` and `DataLoader` object. Here's an overview of what they usually do:\n",
    "\n",
    "`torch.utils.data.Dataset`\n",
    "- Stores dataset (usually a single tensor or list of tensors) inside the object\n",
    "    - Happens in `__init__()` function\n",
    "- Defines how many observations are in the dataset\n",
    "    - In `__len__()`\n",
    "- Defines how to retrieve a single observation from the dataset given its index from 0 inclusive to `__len__()` exclusive\n",
    "    - In `__getitem__()`\n",
    "\n",
    "\n",
    "`torch.utils.data.DataLoader`\n",
    "- Queries and batches observations from an initialized `Dataset`\n",
    "- If `shuffle=True`, shuffles dataset for you every epoch (do this for training, not validation / testing)\n",
    "- Handles basic multiprocessing\n",
    "\n",
    "Some specialized datasets (like in this assignment) usually need a custom `Dataset` class. However, for popular datasets, there are often existing implementations, like those found [here](https://pytorch.org/vision/stable/datasets.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKEqViBhs45N"
   },
   "source": [
    "## Initialize Dataset\n",
    "Fortunately we've already written the custom `Dataset` for you.\n",
    "\n",
    "Open `utils.py`, find `KContextSpectrograms`, and read through it to understand how it works and what initialization parameters are required. Afterwards, complete and run each cell marked with `# TODO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import num_ms\n",
    "\n",
    "# TODO: Specify the desired number of context frames to concatenate to each side of your target,\n",
    "# then run this cell to preview how many milliseconds will be covered with your selected `k`\n",
    "k = 35\n",
    "\n",
    "print(f\"# milliseconds covered with context {k}:\", num_ms(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxQMxHvQs45O"
   },
   "outputs": [],
   "source": [
    "# TODO: Initialize dataset objects for training, validation, and testing.\n",
    "from utils import KContextSpectrograms\n",
    "\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "test_dataset = None\n",
    "### BEGIN SOLUTION\n",
    "train_dataset = KContextSpectrograms(\"data/train.npy\", \"data/train_labels.npy\", k=k)\n",
    "val_dataset = KContextSpectrograms(\"data/val.npy\", \"data/val_labels.npy\", k=k)\n",
    "test_dataset = KContextSpectrograms(\"data/test.npy\", k=k)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DataLoaders\n",
    "Although `Dataset`s frequently need custom implementations, `DataLoader`s are usually standard.\n",
    "\n",
    "Use [this documentation](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) to guide you on implementing the following instructions:\n",
    "\n",
    "- Specify an adequate batch size (see writeup)\n",
    "- Initialize training dataloader with batch size, pinned memory, number of workers, and with shuffling.\n",
    "- Initialize validation dataloader and test dataloader with your batch size, pinned memory, number of workers, and WITHOUT shuffling.\n",
    "    - We don't shuffle val because we're just calculating the accuracy on every observation, so shuffling just slows things down without mattering\n",
    "    - We don't shuffle test because we need to export our predictions in the correct order\n",
    "    - We use num_workers equal to cpu count to prepare data in parallel\n",
    "    - We pin memory to speed up data transfer\n",
    "\n",
    "**Hint**: For examples of initializing `Dataset`s/`DataLoader`s, see [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCfj4p1Q5Omn"
   },
   "outputs": [],
   "source": [
    "# TODO: Specify how many observations should go in each batch\n",
    "batch_size = None\n",
    "### BEGIN SOLUTION\n",
    "batch_size = 128\n",
    "### END SOLUTION\n",
    "# pass num_workers into each dataloader\n",
    "num_workers = os.cpu_count()\n",
    "\n",
    "# TODO: Initialize `Dataloader` objects for training, validation, and testing.\n",
    "train_dataloader = None\n",
    "val_dataloader = None\n",
    "test_dataloader = None\n",
    "### BEGIN SOLUTION\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, pin_memory=True, shuffle=True, num_workers = num_workers)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, pin_memory=True, shuffle=False, num_workers = num_workers)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, pin_memory=True, shuffle=False, num_workers = num_workers)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhQW0BG7s45P"
   },
   "source": [
    "# Section 2: Training/Validation/Prediction Routines\n",
    "\n",
    "Below are a few given methods that should look familiar, but there are some important differences you should try to spot. Read these through carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Given] ALREADY COMPLETED, SHOWN JUST FOR YOUR REFERENCE\n",
    "def train(model, optimizer, scheduler, train_dataloader, val_dataloader, num_epochs):\n",
    "    \"\"\"[Given] Trains and validates network for `num_epochs`\n",
    "\n",
    "    Args:\n",
    "        model (nn.Sequential): Initialized network, stored in an `nn.Sequential` object.\n",
    "        optimizer (optim.Optimizer): Initialized optimizer like `optim.SGD` or `optim.Adam`\n",
    "        scheduler (optim.lr_scheduler): Initialized scheduler like `optim.lr_scheduler.ReduceLROnPlateau` (or None)\n",
    "        train_dataloader (torch.utils.data.DataLoader): Initialized training dataloader\n",
    "        val_dataloader (torch.utils.data.DataLoader): Initialized validation dataloader\n",
    "        num_epochs (int): # epochs to train for\n",
    "    Returns:\n",
    "        list, list: losses is the loss per every batch, val_accuracies is the val accuracy per epoch\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        # No need to manually reshuffle; `Dataloader` handles that for you!\n",
    "        # Train model for one epoch\n",
    "        epoch_losses = train_epoch(model, optimizer, train_dataloader, scheduler)\n",
    "        losses.extend(epoch_losses)\n",
    "        \n",
    "        # Evaluate model on validation set, track accuracy\n",
    "        val_accuracy = validate(model, val_dataloader)\n",
    "        print(100 * val_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    return losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pP-knU9ms45T"
   },
   "outputs": [],
   "source": [
    "# [Given] ALREADY COMPLETED, SHOWN JUST FOR YOUR REFERENCE\n",
    "def validate(model, dataloader):\n",
    "    \"\"\"[Given] Evaluates network and calculates accuracy for a full validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Sequential): Your initialized network, stored in a `Sequential` object.\n",
    "        dataloader (torch.utils.data.DataLoader): Initialized validation dataloader\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy rate for entire val set.\n",
    "    \"\"\"\n",
    "    # Set model to evaluate mode (train mode is `.train()`)\n",
    "    model.eval()\n",
    "\n",
    "    total_correct = 0\n",
    "    # Run loop with `tqdm` progress bar\n",
    "    for i, (data, labels) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Put tensors on specified device (GPU or CPU)\n",
    "        data, labels = data.to(DEVICE), labels.to(DEVICE)\n",
    "        logits = model(data)\n",
    "        num_correct = (logits.argmax(axis=1) == labels).cpu().numpy().sum()\n",
    "        total_correct += num_correct\n",
    "    return total_correct / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aW5b7K4s45Q"
   },
   "source": [
    "## `train_epoch()`\n",
    "\n",
    "Now to write the training routine of a single epoch.\n",
    "\n",
    "See the `validate()` method especially for hints, section 4 [here](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) is also a good reference.\n",
    "\n",
    "```\n",
    "def train_epoch():\n",
    "    set_model_to_train_mode()\n",
    "    loss_function = create_loss_function_object() # cross entropy!\n",
    "    for (data, labels) in tqdm(dataloader):\n",
    "        data, labels = put_tensors_on_appropriate_device(DEVICE, data, labels) # See val method for hint\n",
    "        reset_gradients_to_zero()\n",
    "        logits = forward_pass_through_model(model, data)\n",
    "        loss = loss_function(logits, labels)\n",
    "        run_backprop() # look up how torch does this; it's slightly different from what you did in part A\n",
    "        update_model_params() # using the optimizer\n",
    "        store_loss_value(loss)\n",
    "    return loss_values\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pN7m5dlRs45R"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, dataloader, scheduler=None):\n",
    "    \"\"\"Train model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Sequential): Initialized network, stored in a `nn.Sequential` object.\n",
    "        optimizer (optim.Optimizer): Initialized optimizer like `optim.SGD` or `optim.Adam`\n",
    "        dataloader (torch.utils.data.DataLoader): Initialized training dataloader\n",
    "        scheduler (optim.lr_scheduler): Optional scheduler if you want it\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        list: Loss value of each batch for this epoch.\n",
    "    \"\"\"\n",
    "    # Append loss values to this list.\n",
    "    loss_per_batch = []\n",
    "    \n",
    "    # TODO: Complete code based on pseudocode\n",
    "    ### BEGIN SOLUTION\n",
    "    model.train()\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    # Run loop with `tqdm` progress bar\n",
    "    for i, (data, labels) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # TODO: Complete code based on pseudocode\n",
    "        ### BEGIN SOLUTION\n",
    "        data, labels = data.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(data)\n",
    "        loss = loss_function(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ### END SOLUTION\n",
    "\n",
    "        # Store the loss value for the batch\n",
    "        loss_per_batch.append(loss.item())\n",
    "\n",
    "    # (Feel free to change) If scheduler, determine if we should change LR based on some metric\n",
    "    if scheduler is not None:\n",
    "        scheduler.step(sum(loss_per_batch)) # This assumes ReduceLROnPlateau; the choice of using sum is fairly arbitrary.\n",
    "\n",
    "    return loss_per_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSKQZhYBs45S"
   },
   "source": [
    "## `predict()`\n",
    "\n",
    "This method is very similar to the `validate()` method we gave above. It's used to generate predictions for all observations in the test dataset.\n",
    "\n",
    "You can assume that the dataloader is NOT shuffled. Each batch you receive will have `batch_size` number of observations (no labels), and you want to `extend` a list containing your previous predictions. The end result will be a 1-dimensional list containing integers, the same length as the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pFrOcU6tqVc"
   },
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    \"\"\"Generates predictions for the test dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Sequential): Your initialized network, stored in a `Sequential` object.\n",
    "        dataloader (torch.utils.data.DataLoader): Initialized test dataloader\n",
    "\n",
    "    Returns:\n",
    "        list: should be same length as test dataset, and containing ints (or numpy integers)\n",
    "    \"\"\"\n",
    "    pass\n",
    "    ### BEGIN SOLUTION\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    for i, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        data = data.to(DEVICE)\n",
    "        # TODO: Get the logits from the model, get your batched predictions from your logits, extend your preds list with them\n",
    "        logits = model(data)\n",
    "        predicted = logits.argmax(axis=1)\n",
    "        preds.extend([p.item() for p in predicted])\n",
    "    return preds\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FW3-T-pMs45U"
   },
   "source": [
    "# Section 3: Training\n",
    "\n",
    "You're done with the major coding work! Now just to initialize your model/optimizer and begin training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "rqgdKvPxs45W"
   },
   "outputs": [],
   "source": [
    "# TODO: Initialize your model here\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(20, 15), # just an example model\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(15, 10), # We will need one output for each phoneme (so 10 outputs won't do it);\n",
    "                       # see the writeup for the number of phoenemes we're using!\n",
    ")\n",
    "### BEGIN SOLUTION\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(40*(k * 2 + 1), 1024),\n",
    "    nn.BatchNorm1d(1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.BatchNorm1d(1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.BatchNorm1d(1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 256),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 71)\n",
    ")\n",
    "### END SOLUTION\n",
    "\n",
    "# TODO: put your model on `DEVICE`\n",
    "### BEGIN SOLUTION\n",
    "model = model.to(DEVICE)\n",
    "### END SOLUTION\n",
    "\n",
    "# TODO: Initialize your optimizer\n",
    "optimizer = None\n",
    "### BEGIN SOLUTION\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay = 1e-6)\n",
    "### END SOLUTION\n",
    "\n",
    "# TODO: (optional) Initialize scheduler\n",
    "scheduler = None\n",
    "### BEGIN SOLUTION\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Call your training routine for some epochs (train)\n",
    "num_epochs = 3\n",
    "# losses, val_accuracies = ...\n",
    "### BEGIN SOLUTION\n",
    "losses, val_accuracies = train(model, optimizer, scheduler, train_dataloader, val_dataloader, num_epochs)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [GIVEN] You can plot your training loss progress using this function below.\n",
    "from utils import plot_loss\n",
    "\n",
    "plot_loss(losses, num_batches=len(train_dataloader), num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITPLwKKzs45X"
   },
   "source": [
    "# Section 4: Predict Test Dataset and Submission\n",
    "\n",
    "If you're ready to submit, run these cells below and look for the output file generated in the `submissions/` folder (it will be time-stamped).\n",
    "\n",
    "**NOTE:** The first row of the CSV should look like this:\n",
    "\n",
    "`Id,Category`\n",
    "\n",
    "**Please remember to edit the second entry of the first row ('Category') to the name you want to appear on the leaderboard.**\n",
    "\n",
    "*e.g.* `Id, deepLearner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oo2bEmc9s45Y"
   },
   "outputs": [],
   "source": [
    "from utils import export_predictions_to_csv\n",
    "\n",
    "export_predictions_to_csv(preds)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "-2cBVwfds45L",
    "6ORcUUwWvZOu",
    "6kP0mE8gs45G",
    "mv7pNwQl51uh"
   ],
   "name": "pa_01_B_assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
